---
title: SG Property Price Predictions
author: R package build
date: '2022-12-08'
slug: 2022 sg property price predictions
categories: R
tags: 
- R Markdown
- random forest
- linear regression
- machine learning
---

Continuing from the previous EDA exercise, with same data object. Let us recall back the data structure in our project.

```{r}
salesfile<-"resale-flat-prices-based-on-registration-date-from-jan-2017-onwards.csv"
hdbresale<-read.csv(file=salesfile,header=TRUE)
summary(hdbresale)

library(tidyr)
library(dplyr)
#split column for month into registration year and registration month
hdbresale2<-hdbresale%>%
  separate(month,c("reg_year","reg_month"),"-")
#split remaining_lease to its remaining year and remaining month
hdbresale2<-hdbresale2%>%
  separate(remaining_lease,c("rem_lease_yr","delete1","rem_lease_month","delete2")," ")
hdbresale2<-hdbresale2[,c(-12,-14)]

#number of records
nrow(hdbresale2)

#checking null values
sum(is.na(hdbresale2)) #only rem_lease_month is with nil values, this is because of nil month
sum(is.na(hdbresale2$rem_lease_yr))

#converting columns to its numeric and factor to apply to machine learning algorithm
hdbresale2 <- hdbresale2 %>%
  mutate(reg_year = as.numeric(reg_year),
         reg_month = as.numeric(reg_month),
         flat_type = as.factor(flat_type),
         flat_model = as.factor(flat_model),
         rem_lease_yr = as.numeric(rem_lease_yr),
         storey_range = as.factor(storey_range),
         town = as.factor(town))

```


```{r}
#Display structure of R object. Its indicated that the object is a data frame with 13 variables, and 121,475 rows of transactions.  
str(hdbresale2)
```


```{r}
#plot correlation of the numeric variables
#ggcorr part of GGally packages
#install.packages("GGally") 
library(GGally) #extended version of ggplot2 library
ggcorr(hdbresale2, label = T, hjust = 1, layout.exp = 3)
```

Testing out linear regression model, to show the difference when attempt to overfit the model.

```{r}
#multivariate linear regression on selected variables only
hdb_linear_model=lm(resale_price~reg_year+floor_area_sqm+rem_lease_yr+flat_model, data=hdbresale2)
summary(hdb_linear_model) #rsquare at 0.5689

#multivariate linear regression on all variables
#hdb_linear_model2<-lm(resale_price~.,data=hdbresale2) #  What is considered optimum? Rsquare at 0.9358, 
#summary(hdb_linear_model2)
```

#From doing a split piece. I learnt that when rendering the Rmd file, i cannot allow for a "Install.packages()" code. And also need to source again the raw csv data file. which is why i recopied the earlier few steps from my earlier EDA post.
Also the hdb_linear_model2 was meant to show the result of overfitting the model, but bear in mind the run time for the code is quite long as well, hence for the purpose of quickly saving my render and post it, i simply mark as a comment only. 

I will quickly do a split of data by training data and testing data. After that compare the linear model against the random forest model and compare by root mean square error

```{r}
#split data
set.seed(123)
idx<-sample(nrow(hdbresale2),nrow(hdbresale2)*0.8)
housing_train<-hdbresale2[idx,]
housing_test<-hdbresale2[-idx,]
```

```{r}
#make predictions

housing_test$predict_lm<-predict(hdb_linear_model,housing_test)

```

Random forest model

```{r}
library(randomForest) # for new user of this library, remember to install.packages("randomForest")
hdb_rf_model<- randomForest(resale_price ~reg_year+floor_area_sqm+rem_lease_yr+flat_model, data=housing_train, ntree=4) # try this ntree variable. careful when using AI to solve as it causes lagness or jam
summary(hdb_rf_model)
housing_test$predict_rf<-predict(hdb_rf_model,housing_test)

```

```{r}
# Calculate the root mean squared error for each model
linear_rmse <- sqrt(mean((housing_test$resale_price - housing_test$predict_lm)^2))
rf_rmse <- sqrt(mean((housing_test[, "resale_price"] - housing_test[,"predict_rf"])^2)) #this one is smaller
linear_rmse
rf_rmse
```

Conclusion, you can see that random forest is better coming from the result of lower rmse. However, i note that my predictive modeling here need further refinement, as in to show hyperparameter tuning of a random forest model. I would create a different post to have a better structure of managing a predictive model. (Ie. Setting a recipe, baking, fit, analyze result)
